{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import psycopg2\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('rnn_train/chenmark.csv')\n",
    "\n",
    "# drop the first contest--too much variance\n",
    "data.sort_values('ratingupdatetimeseconds', inplace=True)\n",
    "firstcid = data.contestid.values[0]\n",
    "\n",
    "#data.reset_index(inplace=True)\n",
    "data.drop(data.index[data.contestid == firstcid], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "con = psycopg2.connect(database='codeforces', user='Joy')\n",
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## binarize some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur.execute(\"select * from all_participanttypes\", con)\n",
    "all_part = [c[1] for c in cur.fetchall()]\n",
    "\n",
    "cur.execute(\"select * from all_tags\", con)\n",
    "all_tags = [c[1] for c in cur.fetchall()]\n",
    "\n",
    "cur.execute(\"select * from all_language\", con)\n",
    "all_lang = [c[1] for c in cur.fetchall()]\n",
    "\n",
    "#cur.execute(\"select * from all_verdicts\", con)\n",
    "#all_verd = [c[1] for c in cur.fetchall()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set binary columns to binary, some of them were counts by mistake\n",
    "bin_vars = all_part + all_tags + all_lang\n",
    "data[bin_vars] = data[bin_vars].fillna(value=0)\n",
    "\n",
    "for b in bin_vars:\n",
    "    data.loc[ data[b] > 0, b] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHALLENGED</th>\n",
       "      <th>COMPILATION_ERROR</th>\n",
       "      <th>CONTESTANT</th>\n",
       "      <th>CRASHED</th>\n",
       "      <th>GNU C++</th>\n",
       "      <th>GNU C++11</th>\n",
       "      <th>GYM</th>\n",
       "      <th>MEMORY_LIMIT_EXCEEDED</th>\n",
       "      <th>MS C++</th>\n",
       "      <th>Mysterious Language</th>\n",
       "      <th>...</th>\n",
       "      <th>FALSE</th>\n",
       "      <th>Mono C#</th>\n",
       "      <th>Java 7</th>\n",
       "      <th>Tcl</th>\n",
       "      <th>Haskell</th>\n",
       "      <th>Cobol</th>\n",
       "      <th>Io</th>\n",
       "      <th>GNU C++0x</th>\n",
       "      <th>GNU C++14</th>\n",
       "      <th>GNU C++11 ZIP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 123 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     CHALLENGED  COMPILATION_ERROR  CONTESTANT  CRASHED  GNU C++  GNU C++11  \\\n",
       "142         NaN                NaN         1.0      NaN      1.0        0.0   \n",
       "137         NaN                NaN         1.0      NaN      1.0        0.0   \n",
       "143         NaN                NaN         1.0      NaN      1.0        0.0   \n",
       "145         NaN                NaN         1.0      NaN      1.0        0.0   \n",
       "146         NaN                NaN         1.0      NaN      1.0        0.0   \n",
       "\n",
       "     GYM  MEMORY_LIMIT_EXCEEDED  MS C++  Mysterious Language      ...        \\\n",
       "142  0.0                    NaN     0.0                  0.0      ...         \n",
       "137  0.0                    NaN     0.0                  0.0      ...         \n",
       "143  0.0                    NaN     0.0                  0.0      ...         \n",
       "145  0.0                    NaN     0.0                  0.0      ...         \n",
       "146  0.0                    NaN     0.0                  0.0      ...         \n",
       "\n",
       "     FALSE  Mono C#  Java 7  Tcl  Haskell  Cobol   Io  GNU C++0x  GNU C++14  \\\n",
       "142    0.0      0.0     0.0  0.0      0.0    0.0  0.0        0.0        0.0   \n",
       "137    0.0      0.0     0.0  0.0      0.0    0.0  0.0        0.0        0.0   \n",
       "143    0.0      0.0     0.0  0.0      0.0    0.0  0.0        0.0        0.0   \n",
       "145    0.0      0.0     0.0  0.0      0.0    0.0  0.0        0.0        0.0   \n",
       "146    0.0      0.0     0.0  0.0      0.0    0.0  0.0        0.0        0.0   \n",
       "\n",
       "     GNU C++11 ZIP  \n",
       "142            0.0  \n",
       "137            0.0  \n",
       "143            0.0  \n",
       "145            0.0  \n",
       "146            0.0  \n",
       "\n",
       "[5 rows x 123 columns]"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHALLENGED\n",
      "COMPILATION_ERROR\n",
      "CONTESTANT\n",
      "CRASHED\n",
      "GNU C++\n",
      "GNU C++11\n",
      "GYM\n",
      "MEMORY_LIMIT_EXCEEDED\n",
      "MS C++\n",
      "Mysterious Language\n",
      "OK\n",
      "OUT_OF_COMPETITION\n",
      "PARTIAL\n",
      "PRACTICE\n",
      "PyPy 2\n",
      "Python 2\n",
      "REJECTED\n",
      "RUNTIME_ERROR\n",
      "SKIPPED\n",
      "TIME_LIMIT_EXCEEDED\n",
      "VIRTUAL\n",
      "WRONG_ANSWER\n",
      "contestid\n",
      "delta_smoothed_1months\n",
      "delta_smoothed_2months\n",
      "delta_smoothed_3months\n",
      "delta_smoothed_4months\n",
      "delta_smoothed_5months\n",
      "handle\n",
      "index\n",
      "newrating\n",
      "oldrating\n",
      "points\n",
      "problem_rating\n",
      "rank\n",
      "ratingupdatetimeseconds\n",
      "smoothed_1months\n",
      "smoothed_2months\n",
      "smoothed_3months\n",
      "smoothed_4months\n",
      "smoothed_5months\n",
      "solvetimeseconds\n",
      "starttimeseconds\n",
      "stoptimeseconds\n",
      "shortest paths\n",
      "2-sat\n",
      "greedy\n",
      "meet-in-the-middle\n",
      "matrices\n",
      "number theory\n",
      "constructive algorithms\n",
      "chinese remainder theorem\n",
      "implementation\n",
      "ternary search\n",
      "schedules\n",
      "dsu\n",
      "dfs and similar\n",
      "graph matchings\n",
      "string suffix structures\n",
      "math\n",
      "probabilities\n",
      "fft\n",
      "divide and conquer\n",
      "two pointers\n",
      "trees\n",
      "data structures\n",
      "flows\n",
      "sortings\n",
      "expression parsing\n",
      "dp\n",
      "hashing\n",
      "bitmasks\n",
      "*special\n",
      "geometry\n",
      "combinatorics\n",
      "graphs\n",
      "brute force\n",
      "games\n",
      "binary search\n",
      "strings\n",
      "PRESENTATION_ERROR\n",
      "TESTING\n",
      "FAILED\n",
      "IDLENESS_LIMIT_EXCEEDED\n",
      "Scala\n",
      "Delphi\n",
      "JavaScript\n",
      "PyPy 3\n",
      "Perl\n",
      "Factor\n",
      "D\n",
      "FPC\n",
      "Ada\n",
      "F#\n",
      "Java 8\n",
      "Picat\n",
      "Ruby\n",
      "Java 8 ZIP\n",
      "Pike\n",
      "GNU C\n",
      "MS C#\n",
      "Python 3\n",
      "Secret_171\n",
      "Kotlin\n",
      "J\n",
      "Ocaml\n",
      "Java 6\n",
      "Go\n",
      "Roco\n",
      "GNU C11\n",
      "Befunge\n",
      "Rust\n",
      "PHP\n",
      "FALSE\n",
      "Mono C#\n",
      "Java 7\n",
      "Tcl\n",
      "Haskell\n",
      "Cobol\n",
      "Io\n",
      "GNU C++0x\n",
      "GNU C++14\n",
      "GNU C++11 ZIP\n"
     ]
    }
   ],
   "source": [
    "for c in data.columns:\n",
    "    print c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Remove unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "month = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_data = data\n",
    "for m in range(1,6):\n",
    "    if m == month:\n",
    "        continue\n",
    "    name1 = \"delta_smoothed_%dmonths\" % m\n",
    "    name2 = \"smoothed_%dmonths\" % m\n",
    "    \n",
    "    df_data.drop([name1, name2], axis=1, inplace=True)\n",
    "\n",
    "df_train = df_data.drop(['handle', 'index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train.fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling and grouping by contest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colname = 'delta_smoothed_%dmonths' % month\n",
    "cols = list(df_train.columns.values)\n",
    "colidx = cols.index(colname)\n",
    "\n",
    "cids = df_train.contestid\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "df_train_scaled = scaler.fit_transform(df_train)\n",
    "df_train_scaled = pd.DataFrame(df_train_scaled)\n",
    "df_train_scaled.columns = cols\n",
    "\n",
    "# add back in cols that should not be scaled\n",
    "df_train_scaled['contestid'] = cids\n",
    "df_train_scaled[colname] = df_train[colname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groups = df_train_scaled.groupby('contestid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainlist = []\n",
    "ylist = []\n",
    "bins = range(-200, 200, 20)\n",
    "\n",
    "for k, v in groups:\n",
    "    base = [0] * (len(bins) + 1)\n",
    "    v.is_copy = False\n",
    "    \n",
    "    v.drop('contestid', axis=1, inplace=True)\n",
    "    y = v.loc[:, colname].values[0]\n",
    "    v.drop(colname, inplace=True, axis=1)\n",
    "    \n",
    "    trainlist.append(v)\n",
    "    ylist.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-23.699999999999999, 65.314285709999993)"
      ]
     },
     "execution_count": 558,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(ylist), max(ylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yvecs = [ [0] * (len(bins) + 1) for i in range(len(ylist))]\n",
    "idx1 = np.digitize(ylist, bins=bins)\n",
    "\n",
    "for i, j in enumerate(idx1):\n",
    "    yvecs[i][j] = 1\n",
    "#yvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ary = np.array(yvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88, 88)"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainlist), len(ylist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad X values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: need to make this \"universal\" across all users\n",
    "maxtimepts = max([len(t) for t in trainlist])\n",
    "size = trainlist[0].shape[1]\n",
    "\n",
    "for i in range(len(trainlist)):\n",
    "    gap = maxtimepts - len(trainlist[i])\n",
    "    for j in range(gap):\n",
    "        nullrow = [0] * size\n",
    "        trainlist[i].loc[-j-1] = nullrow\n",
    "    trainlist[i].sort_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print len(trainlist)\n",
    "#[t.shape for t in trainlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = pd.concat(trainlist)\n",
    "dfx.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "arx = np.array(dfx)\n",
    "arx = np.reshape(arx, (len(trainlist), maxtimepts, 111))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up keras model\n",
    "https://keras.io/layers/recurrent/\n",
    "https://keras.io/getting-started/sequential-model-guide/\n",
    "\n",
    "```keras.layers.recurrent.Recurrent(return_sequences=False, go_backwards=False, stateful=False, unroll=False, implementation=0)```\n",
    "* ```weights```: list of Numpy arrays to set as initial weights. The list should have 3 elements, of shapes: [(input_dim, output_dim), (output_dim, output_dim), (output_dim,)].\n",
    "* ```return_sequences```: Boolean. Whether to return the last output in the output sequence, or the full sequence.\n",
    "* ```go_backwards```: Boolean (default False). If True, process the input sequence backwards and return the reversed sequence.\n",
    "* ```stateful```: Boolean (default False). If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch.\n",
    "* ```unroll```: Boolean (default False). If True, the network will be unrolled, else a symbolic loop will be used. Unrolling can speed-up a RNN, although it tends to be more memory-intensive. Unrolling is only suitable for short sequences.\n",
    "* ```implementation```: one of {0, 1, or 2}. If set to 0, the RNN will use an implementation that uses fewer, larger matrix products, thus running faster on CPU but consuming more memory. If set to 1, the RNN will use more matrix products, but smaller ones, thus running slower (may actually be faster on GPU) while consuming less memory. If set to 2 (LSTM/GRU only), the RNN will combine the input gate, the forget gate and the output gate into a single matrix, enabling more time-efficient parallelization on the GPU.\n",
    "    * Note: RNN dropout must be shared for all gates, resulting in a slightly reduced regularization.\n",
    "* ```input_dim```: dimensionality of the input (integer). This argument (or alternatively, the keyword argument input_shape) is required when using this layer as the first layer in a model.\n",
    "* ```input_length```: Length of input sequences, to be specified when it is constant. This argument is required if you are going to connect  Flatten then Dense layers upstream (without it, the shape of the dense outputs cannot be computed). Note that if the recurrent layer is not the first layer in your model, you would need to specify the input length at the level of the first layer (e.g. via the input_shape argument)\n",
    "\n",
    "**Note on using statefulness in RNNs**\n",
    "\n",
    "You can set RNN layers to be 'stateful', which means that the states computed for the samples in one batch will be reused as initial states for the samples in the next batch. This assumes a one-to-one mapping between samples in different successive batches.\n",
    "\n",
    "To enable statefulness:\n",
    "- specify ```stateful=True``` in the layer constructor. \n",
    "- specify a fixed batch size for your model, by passing if sequential model:  ```batch_input_shape=(...)``` to the first layer in your model.\n",
    "else for functional model with 1 or more Input layers:  ```batch_shape=(...)``` to all the first layers in your model.\n",
    "This is the expected shape of your inputs including the batch size. It should be a tuple of integers, e.g. (32, 10, 100).\n",
    "- specify ```shuffle=False``` when calling ```fit()```.\n",
    "\n",
    "To reset the states of your model, call ```.reset_states()``` on either a specific layer, or on your entire model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE CURRENTLY USING THE WRONG OUTPUT LAYER, NEED REGRESSION NOT CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_user_data(user):\n",
    "    # -----------------------------\n",
    "    # Load data\n",
    "    data = pd.read_csv('rnn_train/%s.csv'%user)\n",
    "\n",
    "    # drop the first contest--too much variance\n",
    "    data.sort_values('ratingupdatetimeseconds', inplace=True)\n",
    "    firstcid = data.contestid.values[0]\n",
    "    data.drop(data.index[data.contestid == firstcid], axis=0, inplace=True)\n",
    "\n",
    "    # -----------------------------\n",
    "    # binarize some variables\n",
    "\n",
    "    cur.execute(\"select * from all_participanttypes\", con)\n",
    "    all_part = [c[1] for c in cur.fetchall()]\n",
    "\n",
    "    cur.execute(\"select * from all_tags\", con)\n",
    "    all_tags = [c[1] for c in cur.fetchall()]\n",
    "\n",
    "    cur.execute(\"select * from all_language\", con)\n",
    "    all_lang = [c[1] for c in cur.fetchall()]\n",
    "\n",
    "    # set binary columns to binary, some of them were counts by mistake\n",
    "    bin_vars = all_part + all_tags + all_lang\n",
    "    data[bin_vars] = data[bin_vars].fillna(value=0)\n",
    "\n",
    "    for b in bin_vars:\n",
    "        data.loc[ data[b] > 0, b] = 1\n",
    "\n",
    "    # -----------------------------\n",
    "    # remove information for other months\n",
    "    df_data = data\n",
    "    for m in range(1,6):\n",
    "        if m == month:\n",
    "            continue\n",
    "        name1 = \"delta_smoothed_%dmonths\" % m\n",
    "        name2 = \"smoothed_%dmonths\" % m\n",
    "        \n",
    "        df_data.drop([name1, name2], axis=1, inplace=True)\n",
    "\n",
    "    df_train = df_data.drop(['handle', 'index'], axis=1)\n",
    "    df_train.fillna(value=0, inplace=True)\n",
    "\n",
    "\n",
    "    # -----------------------------\n",
    "    # Feature scaling and grouping by contest\n",
    "    colname = 'delta_smoothed_%dmonths' % month\n",
    "    cols = list(df_train.columns.values)\n",
    "    colidx = cols.index(colname)\n",
    "\n",
    "    cids = df_train.contestid\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    df_train_scaled = scaler.fit_transform(df_train)\n",
    "    df_train_scaled = pd.DataFrame(df_train_scaled)\n",
    "    df_train_scaled.columns = cols\n",
    "\n",
    "    # add back in cols that should not be scaled\n",
    "    df_train_scaled['contestid'] = cids\n",
    "    df_train_scaled[colname] = df_train[colname]\n",
    "\n",
    "    groups = df_train_scaled.groupby('contestid')\n",
    "\n",
    "    # -----------------------------\n",
    "    # create list of inputs for training\n",
    "    trainlist = []\n",
    "    ylist = []\n",
    "\n",
    "    for k, v in groups:\n",
    "        base = [0] * (len(bins) + 1)\n",
    "        v.is_copy = False\n",
    "        \n",
    "        v.drop('contestid', axis=1, inplace=True)\n",
    "        y = v.loc[:, colname].values[0]\n",
    "        v.drop(colname, inplace=True, axis=1)\n",
    "        \n",
    "        trainlist.append(v)\n",
    "        ylist.append(y)\n",
    "\n",
    "\n",
    "    yvecs = [ [0] * (len(bins) + 1) for i in range(len(ylist))]\n",
    "    idx1 = np.digitize(ylist, bins=bins)\n",
    "    for i, j in enumerate(idx1):\n",
    "        yvecs[i][j] = 1\n",
    "    ary = np.array(yvecs)\n",
    "\n",
    "\n",
    "    # -----------------------------\n",
    "    # Pad X values\n",
    "    # TODO: need to make this \"universal\" across all users\n",
    "    #maxtimepts = max([len(t) for t in trainlist])\n",
    "    size = trainlist[0].shape[1]\n",
    "\n",
    "    for i in range(len(trainlist)):\n",
    "        gap = maxtimepts - len(trainlist[i])\n",
    "        for j in range(gap):\n",
    "            nullrow = [0] * size\n",
    "            trainlist[i].loc[-j-1] = nullrow\n",
    "        trainlist[i].sort_index(inplace = True)\n",
    "\n",
    "\n",
    "    dfx = pd.concat(trainlist)\n",
    "    dfx.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    arx = np.array(dfx)\n",
    "    arx = np.reshape(arx, (len(trainlist), maxtimepts, 111))\n",
    "    return arx, ary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "xx_train, yy_train = get_user_data(\"lewin\")\n",
    "\n",
    "xx_test, yy_test = get_user_data(\"chenmark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110 111 20\n"
     ]
    }
   ],
   "source": [
    "maxtimepts = 110 #max([len(t) for t in trainlist])\n",
    "size = 111 #trainlist[0].shape[1]\n",
    "n_neurons = 10\n",
    "\n",
    "print maxtimepts, size, len(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(layer1, layer2, batch_input_shape):\n",
    "    model = Sequential()\n",
    "    batch_input_shape = (xx_train.shape[0], maxtimepts, size)\n",
    "    model.add(GRU(layer1, return_sequences=True, stateful=True, batch_input_shape=batch_input_shape))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(GRU(layer2, return_sequences=False, stateful=True, batch_input_shape=batch_input_shape))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(len(bins) + 1, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons1 = 4\n",
    "neurons2 = 1\n",
    "model = create_model(neurons1, neurons2, batch_input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_25 (GRU)                 (49, 10)                  3660      \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (49, 10)                  0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (49, 21)                  231       \n",
      "=================================================================\n",
      "Total params: 3,891\n",
      "Trainable params: 3,891\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For a multi-class classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88, 110, 111)\n",
      "(49, 21)\n",
      "(49, 110, 111)\n",
      "(49, 21)\n"
     ]
    }
   ],
   "source": [
    "print batch_input_shape\n",
    "print model.output_shape\n",
    "\n",
    "print xx_train.shape\n",
    "print yy_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "49/49 [==============================] - 6s - loss: 3.0536 - acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s - loss: 3.0434 - acc: 0.1837\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s - loss: 3.0525 - acc: 0.1429\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s - loss: 3.0365 - acc: 0.1429\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s - loss: 3.0273 - acc: 0.1633\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s - loss: 3.0365 - acc: 0.1224\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 0s - loss: 3.0283 - acc: 0.1224\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - 0s - loss: 3.0271 - acc: 0.1837\n",
      "Epoch 9/100\n",
      "49/49 [==============================] - 0s - loss: 3.0397 - acc: 0.1633\n",
      "Epoch 10/100\n",
      "49/49 [==============================] - 0s - loss: 3.0324 - acc: 0.0816\n",
      "Epoch 11/100\n",
      "49/49 [==============================] - 0s - loss: 3.0244 - acc: 0.1429\n",
      "Epoch 12/100\n",
      "49/49 [==============================] - 0s - loss: 3.0248 - acc: 0.0612\n",
      "Epoch 13/100\n",
      "49/49 [==============================] - 0s - loss: 3.0394 - acc: 0.2041\n",
      "Epoch 14/100\n",
      "49/49 [==============================] - 0s - loss: 3.0055 - acc: 0.1837\n",
      "Epoch 15/100\n",
      "49/49 [==============================] - 0s - loss: 3.0182 - acc: 0.2449\n",
      "Epoch 16/100\n",
      "49/49 [==============================] - 0s - loss: 3.0267 - acc: 0.1429\n",
      "Epoch 17/100\n",
      "49/49 [==============================] - 0s - loss: 3.0094 - acc: 0.1429\n",
      "Epoch 18/100\n",
      "49/49 [==============================] - 0s - loss: 3.0160 - acc: 0.1633\n",
      "Epoch 19/100\n",
      "49/49 [==============================] - 0s - loss: 3.0073 - acc: 0.1633\n",
      "Epoch 20/100\n",
      "49/49 [==============================] - 0s - loss: 3.0185 - acc: 0.1020\n",
      "Epoch 21/100\n",
      "49/49 [==============================] - 0s - loss: 2.9919 - acc: 0.2041\n",
      "Epoch 22/100\n",
      "49/49 [==============================] - 0s - loss: 3.0114 - acc: 0.1837\n",
      "Epoch 23/100\n",
      "49/49 [==============================] - 0s - loss: 3.0180 - acc: 0.1429\n",
      "Epoch 24/100\n",
      "49/49 [==============================] - 0s - loss: 3.0115 - acc: 0.1020\n",
      "Epoch 25/100\n",
      "49/49 [==============================] - 0s - loss: 3.0021 - acc: 0.1837\n",
      "Epoch 26/100\n",
      "49/49 [==============================] - 0s - loss: 3.0038 - acc: 0.1020\n",
      "Epoch 27/100\n",
      "49/49 [==============================] - 0s - loss: 3.0040 - acc: 0.2245\n",
      "Epoch 28/100\n",
      "49/49 [==============================] - 0s - loss: 2.9960 - acc: 0.2041\n",
      "Epoch 29/100\n",
      "49/49 [==============================] - 0s - loss: 3.0100 - acc: 0.1224\n",
      "Epoch 30/100\n",
      "49/49 [==============================] - 0s - loss: 3.0037 - acc: 0.1633\n",
      "Epoch 31/100\n",
      "49/49 [==============================] - 0s - loss: 2.9946 - acc: 0.2449\n",
      "Epoch 32/100\n",
      "49/49 [==============================] - 0s - loss: 2.9850 - acc: 0.2041\n",
      "Epoch 33/100\n",
      "49/49 [==============================] - 0s - loss: 2.9997 - acc: 0.1224\n",
      "Epoch 34/100\n",
      "49/49 [==============================] - 0s - loss: 2.9926 - acc: 0.1633\n",
      "Epoch 35/100\n",
      "49/49 [==============================] - 0s - loss: 2.9807 - acc: 0.1633\n",
      "Epoch 36/100\n",
      "49/49 [==============================] - 0s - loss: 2.9906 - acc: 0.1224\n",
      "Epoch 37/100\n",
      "49/49 [==============================] - 0s - loss: 2.9930 - acc: 0.1633\n",
      "Epoch 38/100\n",
      "49/49 [==============================] - 0s - loss: 2.9906 - acc: 0.1633\n",
      "Epoch 39/100\n",
      "49/49 [==============================] - 0s - loss: 2.9826 - acc: 0.2245\n",
      "Epoch 40/100\n",
      "49/49 [==============================] - 0s - loss: 2.9816 - acc: 0.1633\n",
      "Epoch 41/100\n",
      "49/49 [==============================] - 0s - loss: 2.9848 - acc: 0.1633\n",
      "Epoch 42/100\n",
      "49/49 [==============================] - 0s - loss: 2.9676 - acc: 0.2041\n",
      "Epoch 43/100\n",
      "49/49 [==============================] - 0s - loss: 2.9911 - acc: 0.1837\n",
      "Epoch 44/100\n",
      "49/49 [==============================] - 0s - loss: 2.9771 - acc: 0.2041\n",
      "Epoch 45/100\n",
      "49/49 [==============================] - 0s - loss: 2.9557 - acc: 0.2245\n",
      "Epoch 46/100\n",
      "49/49 [==============================] - 0s - loss: 2.9808 - acc: 0.2041\n",
      "Epoch 47/100\n",
      "49/49 [==============================] - 0s - loss: 2.9618 - acc: 0.1837\n",
      "Epoch 48/100\n",
      "49/49 [==============================] - 0s - loss: 2.9897 - acc: 0.1020\n",
      "Epoch 49/100\n",
      "49/49 [==============================] - 0s - loss: 2.9615 - acc: 0.1837\n",
      "Epoch 50/100\n",
      "49/49 [==============================] - 0s - loss: 2.9900 - acc: 0.1429\n",
      "Epoch 51/100\n",
      "49/49 [==============================] - 0s - loss: 2.9754 - acc: 0.1020\n",
      "Epoch 52/100\n",
      "49/49 [==============================] - 0s - loss: 2.9621 - acc: 0.1837\n",
      "Epoch 53/100\n",
      "49/49 [==============================] - 0s - loss: 2.9657 - acc: 0.2041\n",
      "Epoch 54/100\n",
      "49/49 [==============================] - 0s - loss: 2.9568 - acc: 0.2041\n",
      "Epoch 55/100\n",
      "49/49 [==============================] - 0s - loss: 2.9522 - acc: 0.1633\n",
      "Epoch 56/100\n",
      "49/49 [==============================] - 0s - loss: 2.9519 - acc: 0.2041\n",
      "Epoch 57/100\n",
      "49/49 [==============================] - 0s - loss: 2.9450 - acc: 0.1837\n",
      "Epoch 58/100\n",
      "49/49 [==============================] - 0s - loss: 2.9827 - acc: 0.1633\n",
      "Epoch 59/100\n",
      "49/49 [==============================] - 0s - loss: 2.9869 - acc: 0.1837\n",
      "Epoch 60/100\n",
      "49/49 [==============================] - 0s - loss: 2.9659 - acc: 0.2041\n",
      "Epoch 61/100\n",
      "49/49 [==============================] - 0s - loss: 2.9603 - acc: 0.1429\n",
      "Epoch 62/100\n",
      "49/49 [==============================] - 0s - loss: 2.9562 - acc: 0.2041\n",
      "Epoch 63/100\n",
      "49/49 [==============================] - 0s - loss: 2.9439 - acc: 0.2041\n",
      "Epoch 64/100\n",
      "49/49 [==============================] - 0s - loss: 2.9464 - acc: 0.1633\n",
      "Epoch 65/100\n",
      "49/49 [==============================] - 0s - loss: 2.9328 - acc: 0.1837\n",
      "Epoch 66/100\n",
      "49/49 [==============================] - 0s - loss: 2.9409 - acc: 0.1837\n",
      "Epoch 67/100\n",
      "49/49 [==============================] - 0s - loss: 2.9366 - acc: 0.1224\n",
      "Epoch 68/100\n",
      "49/49 [==============================] - 0s - loss: 2.9409 - acc: 0.1633\n",
      "Epoch 69/100\n",
      "49/49 [==============================] - 0s - loss: 2.9318 - acc: 0.1837\n",
      "Epoch 70/100\n",
      "49/49 [==============================] - 0s - loss: 2.9457 - acc: 0.1224\n",
      "Epoch 71/100\n",
      "49/49 [==============================] - 0s - loss: 2.9324 - acc: 0.1633\n",
      "Epoch 72/100\n",
      "49/49 [==============================] - 0s - loss: 2.9315 - acc: 0.1837\n",
      "Epoch 73/100\n",
      "49/49 [==============================] - 0s - loss: 2.9440 - acc: 0.1224\n",
      "Epoch 74/100\n",
      "49/49 [==============================] - 0s - loss: 2.9268 - acc: 0.2041\n",
      "Epoch 75/100\n",
      "49/49 [==============================] - 0s - loss: 2.9419 - acc: 0.1224\n",
      "Epoch 76/100\n",
      "49/49 [==============================] - 0s - loss: 2.9297 - acc: 0.2449\n",
      "Epoch 77/100\n",
      "49/49 [==============================] - 0s - loss: 2.9172 - acc: 0.2041\n",
      "Epoch 78/100\n",
      "49/49 [==============================] - 0s - loss: 2.9439 - acc: 0.1837\n",
      "Epoch 79/100\n",
      "49/49 [==============================] - 0s - loss: 2.9111 - acc: 0.2041\n",
      "Epoch 80/100\n",
      "49/49 [==============================] - 0s - loss: 2.9518 - acc: 0.0816\n",
      "Epoch 81/100\n",
      "49/49 [==============================] - 0s - loss: 2.9242 - acc: 0.1429\n",
      "Epoch 82/100\n",
      "49/49 [==============================] - 0s - loss: 2.9344 - acc: 0.2245\n",
      "Epoch 83/100\n",
      "49/49 [==============================] - 0s - loss: 2.9329 - acc: 0.1633\n",
      "Epoch 84/100\n",
      "49/49 [==============================] - 0s - loss: 2.9248 - acc: 0.0816\n",
      "Epoch 85/100\n",
      "49/49 [==============================] - 0s - loss: 2.9315 - acc: 0.2041\n",
      "Epoch 86/100\n",
      "49/49 [==============================] - 0s - loss: 2.9138 - acc: 0.1837\n",
      "Epoch 87/100\n",
      "49/49 [==============================] - 1s - loss: 2.9214 - acc: 0.1837\n",
      "Epoch 88/100\n",
      "49/49 [==============================] - 0s - loss: 2.9028 - acc: 0.1020\n",
      "Epoch 89/100\n",
      "49/49 [==============================] - 0s - loss: 2.9014 - acc: 0.1429\n",
      "Epoch 90/100\n",
      "49/49 [==============================] - 0s - loss: 2.9132 - acc: 0.1633\n",
      "Epoch 91/100\n",
      "49/49 [==============================] - 0s - loss: 2.9073 - acc: 0.1837\n",
      "Epoch 92/100\n",
      "49/49 [==============================] - 0s - loss: 2.9118 - acc: 0.0612\n",
      "Epoch 93/100\n",
      "49/49 [==============================] - 0s - loss: 2.9390 - acc: 0.1224\n",
      "Epoch 94/100\n",
      "49/49 [==============================] - 0s - loss: 2.9058 - acc: 0.1837\n",
      "Epoch 95/100\n",
      "49/49 [==============================] - 0s - loss: 2.9108 - acc: 0.2041\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s - loss: 2.9110 - acc: 0.2041\n",
      "Epoch 97/100\n",
      "49/49 [==============================] - 0s - loss: 2.9058 - acc: 0.1429\n",
      "Epoch 98/100\n",
      "49/49 [==============================] - 0s - loss: 2.9012 - acc: 0.2041\n",
      "Epoch 99/100\n",
      "49/49 [==============================] - 0s - loss: 2.9027 - acc: 0.2041\n",
      "Epoch 100/100\n",
      "49/49 [==============================] - 0s - loss: 2.9076 - acc: 0.1633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1440e0d10>"
      ]
     },
     "execution_count": 715,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit(self, x, y, batch_size=32, epochs=10, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0)\n",
    "#model.fit(arx, ary, epochs=50, batch_size=len(trainlist), shuffle=False)\n",
    "model.fit(xx_train, yy_train, epochs=100, batch_size=xx_train.shape[0], shuffle=False)\n",
    "#score = model.evaluate(x_test, y_test, batch_size=len(trainlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "newmodel = create_model(neurons1, neurons2, batch_input_shape)\n",
    "old_weights = model.get_weights()\n",
    "newmodel.set_weights(old_weights)\n",
    "\n",
    "newmodel.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s\n",
      "['loss', 'acc']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.4162671566009521, 0.35227271914482117]"
      ]
     },
     "execution_count": 704,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = newmodel.evaluate(xx_test, yy_test, batch_size=xx_test.shape[0])\n",
    "\n",
    "print model.metrics_names\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we overfit the crap out of that one. Let's look at the number of parameters in this system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 4s - loss: 2.3257 - acc: 0.0960     \n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.3010 - acc: 0.1080     - ETA: 0s - loss: 2.3050 - acc: 0.10\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2916 - acc: 0.1280       \n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2832 - acc: 0.1400     \n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2743 - acc: 0.1520     \n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2680 - acc: 0.1650     \n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2594 - acc: 0.1730     \n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2527 - acc: 0.1740     \n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2473 - acc: 0.1850     \n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2401 - acc: 0.1830     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11ed77fd0>"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For a single-input model with 10 classes (categorical classification):\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(10, size=(1000, 1))\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "one_hot_labels = keras.utils.to_categorical(labels, num_classes=10)\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data, one_hot_labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Iter 1280, Minibatch Loss= 2.090559, Training Accuracy= 0.24219\n",
      "Iter 2560, Minibatch Loss= 1.680801, Training Accuracy= 0.39844\n",
      "Iter 3840, Minibatch Loss= 1.353599, Training Accuracy= 0.49219\n",
      "Iter 5120, Minibatch Loss= 1.229820, Training Accuracy= 0.57031\n",
      "Iter 6400, Minibatch Loss= 1.041029, Training Accuracy= 0.64062\n",
      "Iter 7680, Minibatch Loss= 0.937305, Training Accuracy= 0.69531\n",
      "Iter 8960, Minibatch Loss= 0.710454, Training Accuracy= 0.72656\n",
      "Iter 10240, Minibatch Loss= 0.571756, Training Accuracy= 0.84375\n",
      "Iter 11520, Minibatch Loss= 0.564859, Training Accuracy= 0.82812\n",
      "Iter 12800, Minibatch Loss= 0.432062, Training Accuracy= 0.88281\n",
      "Iter 14080, Minibatch Loss= 0.421861, Training Accuracy= 0.87500\n",
      "Iter 15360, Minibatch Loss= 0.367138, Training Accuracy= 0.86719\n",
      "Iter 16640, Minibatch Loss= 0.456024, Training Accuracy= 0.87500\n",
      "Iter 17920, Minibatch Loss= 0.537776, Training Accuracy= 0.84375\n",
      "Iter 19200, Minibatch Loss= 0.413507, Training Accuracy= 0.83594\n",
      "Iter 20480, Minibatch Loss= 0.407764, Training Accuracy= 0.82812\n",
      "Iter 21760, Minibatch Loss= 0.310379, Training Accuracy= 0.87500\n",
      "Iter 23040, Minibatch Loss= 0.347418, Training Accuracy= 0.89844\n",
      "Iter 24320, Minibatch Loss= 0.323128, Training Accuracy= 0.87500\n",
      "Iter 25600, Minibatch Loss= 0.316683, Training Accuracy= 0.91406\n",
      "Iter 26880, Minibatch Loss= 0.203228, Training Accuracy= 0.92969\n",
      "Iter 28160, Minibatch Loss= 0.271395, Training Accuracy= 0.90625\n",
      "Iter 29440, Minibatch Loss= 0.285487, Training Accuracy= 0.90625\n",
      "Iter 30720, Minibatch Loss= 0.254858, Training Accuracy= 0.93750\n",
      "Iter 32000, Minibatch Loss= 0.201062, Training Accuracy= 0.93750\n",
      "Iter 33280, Minibatch Loss= 0.177602, Training Accuracy= 0.96875\n",
      "Iter 34560, Minibatch Loss= 0.216345, Training Accuracy= 0.93750\n",
      "Iter 35840, Minibatch Loss= 0.180034, Training Accuracy= 0.94531\n",
      "Iter 37120, Minibatch Loss= 0.198506, Training Accuracy= 0.92188\n",
      "Iter 38400, Minibatch Loss= 0.195790, Training Accuracy= 0.92969\n",
      "Iter 39680, Minibatch Loss= 0.105841, Training Accuracy= 0.96875\n",
      "Iter 40960, Minibatch Loss= 0.291270, Training Accuracy= 0.93750\n",
      "Iter 42240, Minibatch Loss= 0.142573, Training Accuracy= 0.96875\n",
      "Iter 43520, Minibatch Loss= 0.176773, Training Accuracy= 0.92188\n",
      "Iter 44800, Minibatch Loss= 0.252339, Training Accuracy= 0.92188\n",
      "Iter 46080, Minibatch Loss= 0.155071, Training Accuracy= 0.95312\n",
      "Iter 47360, Minibatch Loss= 0.165568, Training Accuracy= 0.96094\n",
      "Iter 48640, Minibatch Loss= 0.170596, Training Accuracy= 0.92188\n",
      "Iter 49920, Minibatch Loss= 0.167398, Training Accuracy= 0.93750\n",
      "Iter 51200, Minibatch Loss= 0.181336, Training Accuracy= 0.94531\n",
      "Iter 52480, Minibatch Loss= 0.119596, Training Accuracy= 0.96094\n",
      "Iter 53760, Minibatch Loss= 0.148900, Training Accuracy= 0.92969\n",
      "Iter 55040, Minibatch Loss= 0.240501, Training Accuracy= 0.92969\n",
      "Iter 56320, Minibatch Loss= 0.177877, Training Accuracy= 0.96875\n",
      "Iter 57600, Minibatch Loss= 0.261799, Training Accuracy= 0.91406\n",
      "Iter 58880, Minibatch Loss= 0.163161, Training Accuracy= 0.93750\n",
      "Iter 60160, Minibatch Loss= 0.187381, Training Accuracy= 0.93750\n",
      "Iter 61440, Minibatch Loss= 0.154509, Training Accuracy= 0.94531\n",
      "Iter 62720, Minibatch Loss= 0.192258, Training Accuracy= 0.95312\n",
      "Iter 64000, Minibatch Loss= 0.168498, Training Accuracy= 0.95312\n",
      "Iter 65280, Minibatch Loss= 0.213515, Training Accuracy= 0.92969\n",
      "Iter 66560, Minibatch Loss= 0.208743, Training Accuracy= 0.95312\n",
      "Iter 67840, Minibatch Loss= 0.158352, Training Accuracy= 0.95312\n",
      "Iter 69120, Minibatch Loss= 0.075048, Training Accuracy= 0.97656\n",
      "Iter 70400, Minibatch Loss= 0.112927, Training Accuracy= 0.95312\n",
      "Iter 71680, Minibatch Loss= 0.132778, Training Accuracy= 0.96094\n",
      "Iter 72960, Minibatch Loss= 0.072853, Training Accuracy= 0.97656\n",
      "Iter 74240, Minibatch Loss= 0.099017, Training Accuracy= 0.97656\n",
      "Iter 75520, Minibatch Loss= 0.122713, Training Accuracy= 0.96094\n",
      "Iter 76800, Minibatch Loss= 0.113816, Training Accuracy= 0.97656\n",
      "Iter 78080, Minibatch Loss= 0.135227, Training Accuracy= 0.92969\n",
      "Iter 79360, Minibatch Loss= 0.123757, Training Accuracy= 0.94531\n",
      "Iter 80640, Minibatch Loss= 0.133235, Training Accuracy= 0.96094\n",
      "Iter 81920, Minibatch Loss= 0.208105, Training Accuracy= 0.94531\n",
      "Iter 83200, Minibatch Loss= 0.064377, Training Accuracy= 0.98438\n",
      "Iter 84480, Minibatch Loss= 0.124484, Training Accuracy= 0.96094\n",
      "Iter 85760, Minibatch Loss= 0.140094, Training Accuracy= 0.95312\n",
      "Iter 87040, Minibatch Loss= 0.085213, Training Accuracy= 0.96875\n",
      "Iter 88320, Minibatch Loss= 0.074183, Training Accuracy= 0.97656\n",
      "Iter 89600, Minibatch Loss= 0.065423, Training Accuracy= 0.97656\n",
      "Iter 90880, Minibatch Loss= 0.082192, Training Accuracy= 0.98438\n",
      "Iter 92160, Minibatch Loss= 0.053418, Training Accuracy= 0.98438\n",
      "Iter 93440, Minibatch Loss= 0.059600, Training Accuracy= 0.98438\n",
      "Iter 94720, Minibatch Loss= 0.143425, Training Accuracy= 0.94531\n",
      "Iter 96000, Minibatch Loss= 0.210756, Training Accuracy= 0.95312\n",
      "Iter 97280, Minibatch Loss= 0.132850, Training Accuracy= 0.95312\n",
      "Iter 98560, Minibatch Loss= 0.113217, Training Accuracy= 0.95312\n",
      "Iter 99840, Minibatch Loss= 0.116832, Training Accuracy= 0.94531\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.984375\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "Long Short Term Memory paper: http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\n",
    "Author: Aymeric Damien\n",
    "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "'''\n",
    "To classify images using a recurrent neural network, we consider every image\n",
    "row as a sequence of pixels. Because MNIST image shape is 28*28px, we will then\n",
    "handle 28 sequences of 28 steps for every sample.\n",
    "'''\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 28 # MNIST data input (img shape: 28*28)\n",
    "n_steps = 28 # timesteps\n",
    "n_hidden = 128 # hidden layer num of features\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, n_steps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "pred = RNN(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_len = 128\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_data, y: test_label}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Split dataset (not relevant yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_Y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print X_train.shape\n",
    "print y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### as the first layer in a Sequential model\n",
    "##model = Sequential()\n",
    "##model.add(LSTM(32, input_shape=(10, 64)))\n",
    "### now model.output_shape == (None, 32)\n",
    "### note: `None` is the batch dimension.\n",
    "##\n",
    "### for subsequent layers, no need to specify the input size:\n",
    "##model.add(LSTM(16))\n",
    "#\n",
    "## to stack recurrent layers, you must use return_sequences=True\n",
    "## on any recurrent layer that feeds into another recurrent layer.\n",
    "## note that you only need to specify the input size on the first layer.\n",
    "#model = Sequential()\n",
    "##model.add(Embedding(max_features, output_dim=50))\n",
    "#\n",
    "#batch_input_shape = (xx_train.shape[0], maxtimepts, size)\n",
    "#model.add(GRU(n_neurons, return_sequences=False, stateful=True, batch_input_shape=batch_input_shape))\n",
    "#model.add(Dropout(0.5))\n",
    "##model.add(LSTM(32, return_sequences=False, stateful=True))\n",
    "##model.add(Dropout(0.5))\n",
    "#model.add(Dense(len(bins) + 1, activation='softmax'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_section_display": "none",
   "toc_threshold": 6,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
